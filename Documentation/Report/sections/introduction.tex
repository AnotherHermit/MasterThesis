\chapter{Introduction}\label{cha:intro}

\section{Motivation}

One of the most important parts of making a 3D scene look realistic is the illumination. It grounds objects in the scene and gives a lot of depth information. Even though realistic illumination has been a goal of 3D rendering since the beginning of computer graphics there is still a lot of work to be done, especially when it comes to real time solutions.

Illumination of a scene in computer graphics is a very computationally expensive task. In order to make a scene render in real time a lot of development has been made to get effects that approximate certain effects without the expensive calculations.

For example there are many different variants of \gls{ao} to approximate where in a scene there should be less light if the light was bouncing. Other techniques precalculate advanced lighting effects for static objects and store textures or other look-up-tables for quick access later. While many of these techniques make it possible to interact with scenes that would otherwise be static or stuttering, they either have visible errors or are not possible for dynamic objects or lights.

\gls{gi} tries to simulate correct lighting in a scene without using separate methods for certain effects. Rendering using \gls{pm} makes it possible to create direct and indirect light (both diffuse and specular), caustics and shadows making it one of the more popular techniques for off-line rendering when high quality is needed because it converges toward a correct solution when more photons are used.

However, most solutions for \gls{gi}~\cite{sotagi} does not run in real-time, but in the recent years due to hardware performance increase and novel methods, real time solutions to \gls{gi} has been demonstrated on high-end hardware. Since the hardware in mobile devices are making great progress (still far from high-end desktop solutions) and since it has already been proven once~\cite{gimobile} it would be interesting to see how far the latest mobile generation can push global illumination on limited hardware.

This is especially interesting since \gls{vr} and \gls{ar} are making an entrance to the entertainment scene. It would be of huge value if the devices that we put on our heads manage to produce impressive graphics without having to rely on server side graphics like described in~\cite{cloudlight}. While the idea of calculating graphics and lighting on a server and basically streaming the content to clients is very interesting, it does have a lot of limiting factors mainly when it comes to network connection and mobility. Investigating where the limit is on this generations mobile devices is therefore still an important question.

\section{Purpose}

\section{Problem Statement}

The problem statements for this thesis work will be:

\begin{itemize}
  \item \textit{How far can we take \gls{gi} on mobile devices using \gls{vct}?}
\end{itemize}

Previous results from~\cite{gimobile} show that \gls{gi} in real-time is possible on mobile devices using \glspl{vpl}. Even though the results are quite recent the pace of improvement in mobile computational power means that there should be room for more advanced approaches, like \gls{vct}.

\begin{itemize}
  \item \textit{Does \gls{vct} scale well enough to be used on limited hardware such as a mobile device?}
\end{itemize}

\gls{vct} have parameters that can be tuned between speed and quality. The number of cones and their angles makes a large difference for the result both in the quality and speed but also different effects. The voxelization in particular can make a large difference for the performance, from using a static scene and no per frame updates to voxelizing the complete scene per frame.

\begin{itemize}
  \item \textit{What are the limiting factors of the mobile device? And are there any potential benefits on using mobile devices for \gls{gi}?}
\end{itemize}

The hardware structure on the mobile platforms are quite different from desktop hardware. Using a combined memory model and very limited shared memory (if any) between \gls{gpu} cores makes it necessary to adapt the code and the algorithms used.

\section{Delimitations}

Thesis work is limited to 20 weeks, this means that there will be some delimitations described below in this section.

Android is the mobile platform of choice, since it supports both \gls{ogles}, \gls{ocl} and Vulcan. Android \acrshort{api} level 23 will be used partly because only the later versions have support for \gls{ogles} 3.1+ (requires API 21+) and Vulcan (API level 23+) but it also means no time have to be spent on supporting older functionality in Android.

Development of the algorithm will be done using \gls{ogles} 3.1 + \gls{aep} This should allow for faster development and since the algorithm is mainly \gls{gpu} bound it would not benefit that much from using Vulcan.

The algorithm will be evaluated on a Samsung S7 Edge which features the Mali T880 MP12 \gls{gpu}, if possible it would be great to compare performance on a device with a Snapdragon 820 chip-set featuring the Adreno 530 \gls{gpu}, which are the two latest \glspl{gpu} on the mobile market. Unfortunately iOS does not support \gls{ogles} past 3.0 and also does not have support for geometry shaders in Metal which is required for the suggested approach.

The first priority for the \gls{vct} algorithm will be speed, so that a scene of reasonable size can be rendered in 30 fps on latest mobile hardware, using \gls{vct} for \gls{ao}, direct light, soft shadows, indirect light and specular light. The second priority will be to make the scene as dynamic as possible, allowing both dynamic lights and dynamic objects. The third priority will be to decrease any potential graphical glitches and increase the visual quality of the scene. 